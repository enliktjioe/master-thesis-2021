{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recorded-signal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\enlik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\enlik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "# We need this dataset in order to use the tokenizer\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Also download the list of stopwords to filter out\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def process_text(text):\n",
    "    # Make all the strings lowercase and remove non alphabetic characters\n",
    "    text = re.sub('[^A-Za-z]', ' ', text.lower())\n",
    "\n",
    "    # Tokenize the text; this is, separate every sentence into a list of words\n",
    "    # Since the text is already split into sentences you don't have to call sent_tokenize\n",
    "    tokenized_text = word_tokenize(text)\n",
    "\n",
    "    # Remove the stopwords and stem each word to its root\n",
    "    clean_text = [\n",
    "        stemmer.stem(word) for word in tokenized_text\n",
    "        if word not in stopwords.words('english')\n",
    "    ]\n",
    "\n",
    "    # Remember, this final output is a list of words\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "three-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# bolt_google_playstore_review.csv 17MB (21 Dec 2020)\n",
    "url='https://drive.google.com/file/d/1qWuyf3UrpaU5xnxLmO3GMFa6zybSFYQh/view?usp=sharing'\n",
    "url2='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "df = pd.read_csv(url2, usecols=['content','at'])\n",
    "\n",
    "# remove emoji\n",
    "# https://stackoverflow.com/a/57514515/2670476\n",
    "df = df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "\n",
    "df.content = df.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.content).lower(), 1)\n",
    "df.content = df.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.content.split())), 1)\n",
    "df.content = df.apply(lambda row: \" \".join(re.sub(\"[^a-zA-Z]+\", \" \", row.content).split()), 1)\n",
    "texts = df.content.tolist()\n",
    "timestamps = df['at'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cognitive-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "texts = [process_text(text) for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "monetary-match",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.160*\"good\" + 0.072*\"great\" + 0.058*\"app\"')\n",
      "(1, '0.032*\"driver\" + 0.030*\"app\" + 0.015*\"use\"')\n",
      "(2, '0.022*\"use\" + 0.020*\"easi\" + 0.016*\"like\"')\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "model = models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = model.print_topics(num_words=3)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "usual-caribbean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.061*\"driver\" + 0.036*\"time\" + 0.026*\"ride\"')\n",
      "(1, '0.025*\"ok\" + 0.016*\"de\" + 0.013*\"bad\"')\n",
      "(2, '0.245*\"good\" + 0.111*\"great\" + 0.067*\"app\"')\n",
      "(3, '0.039*\"app\" + 0.020*\"use\" + 0.017*\"taxifi\"')\n",
      "(4, '0.105*\"love\" + 0.103*\"nice\" + 0.041*\"cool\"')\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "model = models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = model.print_topics(num_words=3)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-assessment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-legislature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-event",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
