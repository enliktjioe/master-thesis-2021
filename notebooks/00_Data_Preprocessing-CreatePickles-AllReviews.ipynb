{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified from KEFE's preprocess_review.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.6\n",
      "/Users/enlik/GitRepo/master-thesis-2021/notebooks\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/enlik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from pprint import pprint\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "config = get_config('config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine All Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\n",
      "Total bolt_apple reviews: 3154 \n",
      "\n",
      "google\n",
      "Total bolt_google reviews: 51907 \n",
      "\n",
      "apple\n",
      "Total uber_apple reviews: 10342 \n",
      "\n",
      "google\n",
      "Total uber_google reviews: 10000 \n",
      "\n",
      "apple\n",
      "Total blablacar_apple reviews: 23308 \n",
      "\n",
      "google\n",
      "Total blablacar_google reviews: 21172 \n",
      "\n",
      "apple\n",
      "Total cabify_apple reviews: 7384 \n",
      "\n",
      "google\n",
      "Total cabify_google reviews: 3261 \n",
      "\n",
      "apple\n",
      "Total via_apple reviews: 2392 \n",
      "\n",
      "google\n",
      "Total via_google reviews: 1873 \n",
      "\n",
      "apple\n",
      "Total getaround_apple reviews: 2488 \n",
      "\n",
      "google\n",
      "Total getaround_google reviews: 731 \n",
      "\n",
      "apple\n",
      "Total olacabs_apple reviews: 922 \n",
      "\n",
      "google\n",
      "Total olacabs_google reviews: 10000 \n",
      "\n",
      "apple\n",
      "Total taxieu_apple reviews: 564 \n",
      "\n",
      "google\n",
      "Total taxieu_google reviews: 211 \n",
      "\n",
      "apple\n",
      "Total freenow_apple reviews: 14350 \n",
      "\n",
      "google\n",
      "Total freenow_google reviews: 11078 \n",
      "\n",
      "apple\n",
      "Total yandexgo_apple reviews: 171 \n",
      "\n",
      "google\n",
      "Total yandexgo_google reviews: 7053 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in config['csv_input_local']:\n",
    "    if 'apple' in i:\n",
    "        print('apple')\n",
    "        df = pd.read_csv(config['csv_input_local'][i], index_col=0)\n",
    "        new = df.review.astype(str)\n",
    "        total_reviews = len(new)\n",
    "        print(f'Total {i} reviews: {total_reviews} \\n')\n",
    "    elif 'google' in i:\n",
    "        print('google')\n",
    "        df = pd.read_csv(config['csv_input_local'][i], index_col=0)\n",
    "        new = df.content.astype(str)\n",
    "        total_reviews = len(new)\n",
    "        print(f'Total {i} reviews: {total_reviews} \\n')\n",
    "    else:\n",
    "        print(\"Oops!  That was no valid input.  Try again...\")\n",
    "        \n",
    "    df_merged = df_merged.append(new, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from Google Drive\n",
    "\n",
    "- Google Drive has limitation after multiple access in short time that cause **HTTPError: HTTP Error 403: Forbidden**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in config['csv_input']:\n",
    "#     if 'apple' in i:\n",
    "#         print('apple')\n",
    "#         df = read_csv_from_gdrive(config['csv_input'][i])\n",
    "#         new = df.review.astype(str)\n",
    "#         total_reviews = len(df_merged)\n",
    "#         print(f'Total English reviews: {total_reviews} \\n')\n",
    "#     elif 'google' in i:\n",
    "#         print('google')\n",
    "#         df = read_csv_from_gdrive(config['csv_input'][i])\n",
    "#         new = df.content.astype(str)\n",
    "#     else:\n",
    "#         print(\"Oops!  That was no valid input.  Try again...\")\n",
    "        \n",
    "#     df_merged = df_merged.append(new, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         I love bolt. I don’t use uber often because on...\n",
       "1         So annoyed with this app!! Definitely the wors...\n",
       "2         I’ve been using bolt for a month now. I had a ...\n",
       "3         To make things clear, I am not a regular revie...\n",
       "4         I have used the app 3 or 4 times and I thought...\n",
       "                                ...                        \n",
       "182356                                                 good\n",
       "182357                                                 nice\n",
       "182358                                                 Good\n",
       "182359                                            Excellent\n",
       "182360                                                 Good\n",
       "Length: 182361, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English reviews: 182361 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_reviews = len(df_merged)\n",
    "\n",
    "print(f'Total English reviews: {total_reviews} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_docs = remove_things(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_of_words = list(sentences_to_words(cleaned_docs))\n",
    "lists_of_words_no_stops = remove_stopwords(lists_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making bigrams...\n"
     ]
    }
   ],
   "source": [
    "ngrams = make_bigrams(lists_of_words_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing...\n"
     ]
    }
   ],
   "source": [
    "data_lemmatized = lemmatize(ngrams, allowed_postags=['NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.Dictionary(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "term_doc = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(term_doc[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in term_doc[:1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = models.TfidfModel(term_doc, smartirs='ntc')[term_doc]\n",
    "tf_idf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in tf_idf[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save pre-processed data into binary Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "output_path = 'preprocessed_data/all/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path + 'data_lemmatized.pkl', 'wb') as f:\n",
    "    pickle.dump(data_lemmatized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('preprocessed_data/data_lemmatized.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path + 'dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(id2word, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path + 'term_doc.pkl', 'wb') as f:\n",
    "    pickle.dump(term_doc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path + 'tf_idf.pkl', 'wb') as f:\n",
    "    pickle.dump(tf_idf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save df_merged into CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(output_path + 'all_10_apps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
