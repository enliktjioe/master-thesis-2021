{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified from KEFE's preprocess_review.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.3 :: Anaconda, Inc.\n",
      "/gpfs/space/home/enlik/GitRepo/master-thesis-2021/notebooks\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /gpfs/space/home/enlik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/gpfs/space/home/enlik/.local/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from pprint import pprint\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "config = get_config('config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bolt (Both Apple App Store and Google Play Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English reviews: 51907 \n",
      "\n",
      "Total unique users : 46872\n",
      "Total unknown users: 4780\n",
      "Total users who gave multiple reviews: 255\n",
      "\n",
      "Average rating for this app based on the textual reviews: 3.96 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = read_csv_from_gdrive(config['csv_input']['bolt_google'])\n",
    "total_reviews = len(df)\n",
    "unique_users  = len(df['userName'].unique())\n",
    "unknown_users = len(df[df['userName']=='A Google user'])\n",
    "mean = df['score'].mean()\n",
    "\n",
    "print(f'Total English reviews: {total_reviews} \\n')\n",
    "print(f'Total unique users : {unique_users}')\n",
    "print(f'Total unknown users: {unknown_users}')\n",
    "print(f'Total users who gave multiple reviews: {total_reviews - unique_users - unknown_users}\\n')\n",
    "print(f'Average rating for this app based on the textual reviews: {round(mean,2)} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English reviews: 3154 \n",
      "\n",
      "Total unique users : 3149\n",
      "Total users who gave multiple reviews: 5\n",
      "\n",
      "Average rating for this app based on the textual reviews: 3.02 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = read_csv_from_gdrive(config['csv_input']['bolt_apple'])\n",
    "total_reviews = len(df2)\n",
    "unique_users  = len(df2['userName'].unique())\n",
    "mean = df2['rating'].mean()\n",
    "\n",
    "print(f'Total English reviews: {total_reviews} \\n')\n",
    "print(f'Total unique users : {unique_users}')\n",
    "print(f'Total users who gave multiple reviews: {total_reviews - unique_users}\\n')\n",
    "print(f'Average rating for this app based on the textual reviews: {round(mean,2)} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English reviews: 55061 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge both apple and google\n",
    "google = df.content\n",
    "apple = df2.review\n",
    "bolt_reviews = google.append(apple)\n",
    "total_reviews = len(bolt_reviews)\n",
    "\n",
    "print(f'Total English reviews: {total_reviews} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 22:31:07,464 : ERROR : Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/storage/software/python-3.6.3/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-54edec6e7e09>\", line 1, in <module>\n",
      "    cleaned_docs = remove_things(bolt_reviews)\n",
      "  File \"/gpfs/space/home/enlik/GitRepo/master-thesis-2021/notebooks/utils.py\", line 109, in remove_things\n",
      "    return text.map(remove_digits_lower).map(remove_punc).map(remove_repeats)\n",
      "  File \"/gpfs/space/home/enlik/.local/lib/python3.6/site-packages/pandas/core/series.py\", line 3630, in map\n",
      "    new_values = super()._map_values(arg, na_action=na_action)\n",
      "  File \"/gpfs/space/home/enlik/.local/lib/python3.6/site-packages/pandas/core/base.py\", line 1145, in _map_values\n",
      "    new_values = map_f(values, mapper)\n",
      "  File \"pandas/_libs/lib.pyx\", line 2329, in pandas._libs.lib.map_infer\n",
      "  File \"/gpfs/space/home/enlik/GitRepo/master-thesis-2021/notebooks/utils.py\", line 106, in <lambda>\n",
      "    remove_digits_lower = lambda x: re.sub('\\w*\\d\\w*', ' ', x.lower())\n",
      "AttributeError: 'float' object has no attribute 'lower'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/software/python-3.6.3/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/software/python-3.6.3/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/storage/software/python-3.6.3/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/storage/software/python-3.6.3/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/storage/software/python-3.6.3/miniconda3/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/storage/software/python-3.6.3/miniconda3/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/storage/software/python-3.6.3/miniconda3/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/storage/software/python-3.6.3/miniconda3/lib/python3.6/inspect.py\", line 730, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/gpfs/space/home/enlik/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/gpfs/space/home/enlik/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/storage/software/python-3.6.3/miniconda3/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/gpfs/space/home/enlik/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 28, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n",
      "  File \"/gpfs/space/home/enlik/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/gpfs/space/home/enlik/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/storage/software/python-3.6.3/miniconda3/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.python'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "cleaned_docs = remove_things(bolt_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_of_words = list(sentences_to_words(cleaned_docs))\n",
    "lists_of_words_no_stops = remove_stopwords(lists_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = make_bigrams(lists_of_words_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = lemmatize(ngrams, allowed_postags=['NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.Dictionary(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "term_doc = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(term_doc[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in term_doc[:1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = models.TfidfModel(term_doc, smartirs='ntc')[term_doc]\n",
    "tf_idf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in tf_idf[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save pre-processed data into binary Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "output_path = 'preprocessed_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path + 'data_lemmatized.pkl', 'wb') as f:\n",
    "    pickle.dump(data_lemmatized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('preprocessed_data/data_lemmatized.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path + 'dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(id2word, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path + 'term_doc.pkl', 'wb') as f:\n",
    "    pickle.dump(term_doc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path + 'tf_idf.pkl', 'wb') as f:\n",
    "    pickle.dump(tf_idf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "th_python3.8",
   "language": "python",
   "name": "th_python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
