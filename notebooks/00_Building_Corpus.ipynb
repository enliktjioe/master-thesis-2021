{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "integrated-filing",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library\n",
    "- https://www.tutorialspoint.com/gensim/gensim_creating_a_dictionary.htm\n",
    "- https://github.com/fiyero/LDA_gensim\n",
    "- https://github.com/fiyero/LDA_gensim/blob/master/LDA%20with%20Gensim_git.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-associate",
   "metadata": {},
   "source": [
    "## Raw Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "portuguese-parent",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\enlik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from pprint import pprint\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "config = get_config('config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "republican-compression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English reviews: 3154 \n",
      "\n",
      "Total unique users : 3149\n",
      "Total users who gave multiple reviews: 5\n",
      "\n",
      "Average rating for this app based on the textual reviews: 3.02 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = read_csv_from_gdrive(config['csv_input']['bolt_apple'])\n",
    "total_reviews = len(df2)\n",
    "unique_users  = len(df2['userName'].unique())\n",
    "mean = df2['rating'].mean()\n",
    "\n",
    "print(f'Total English reviews: {total_reviews} \\n')\n",
    "print(f'Total unique users : {unique_users}')\n",
    "print(f'Total users who gave multiple reviews: {total_reviews - unique_users}\\n')\n",
    "print(f'Average rating for this app based on the textual reviews: {round(mean,2)} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "herbal-firmware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       I love bolt. I don‚Äôt use uber often because on...\n",
       "1       So annoyed with this app!! Definitely the wors...\n",
       "2       I‚Äôve been using bolt for a month now. I had a ...\n",
       "3       To make things clear, I am not a regular revie...\n",
       "4       I have used the app 3 or 4 times and I thought...\n",
       "                              ...                        \n",
       "3149    –ó–∞—á–µ–º —Ä–µ–≥–µ—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è? –Ø –ø—Ä–æ—Å—Ç–æ —Ö–æ—á—É –ø–æ—Å–º–æ—Ç—Ä–µ...\n",
       "3150                                                   :)\n",
       "3151    –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –º–∞—à–∏–Ω, –∏–Ω–æ–≥–¥–∞ –Ω–µ—Ç –≤–æ–æ–±—â–µ, —á–∞—Å—Ç–æ –ø...\n",
       "3152    1. samm: positsioneerime telefoni\\n2. samm: \"V...\n",
       "3153    Taxify's purpose was that you can CHOOSE your ...\n",
       "Name: review, Length: 3154, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple = df2.review.astype(str)\n",
    "apple = apple.reset_index(drop=True)\n",
    "apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-nickname",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "strategic-sellers",
   "metadata": {},
   "source": [
    "## Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "clear-block",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love bolt. I don‚Äôt use uber often because one ride even if it‚Äôs short is like ¬£11. Whereas from my high street to my house is ¬£3. Not only that but their drivers are SUPER friendly! I was sick one day (my first time using bolt) and the driver was so understanding and encouraged me throughout my journey. Bolt is 100% recommended by me. I don‚Äôt write reviews so that‚Äôs how you know I defiantly recommend it. My Instagram name is: TeeKezi if you wish to get in contact with me about bolt. I‚Äôm not an ambassador üòÇ or anything like that. Just super happy with the service I have been receiving x'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = apple[0]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "australian-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['bolt','taxify','uber','blablacar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "marked-technology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'use',\n",
       " 'often',\n",
       " 'one',\n",
       " 'ride',\n",
       " 'even',\n",
       " 'short',\n",
       " 'like',\n",
       " 'whereas',\n",
       " 'high',\n",
       " 'street',\n",
       " 'house',\n",
       " 'drivers',\n",
       " 'super',\n",
       " 'friendly',\n",
       " 'sick',\n",
       " 'one',\n",
       " 'day',\n",
       " 'first',\n",
       " 'time',\n",
       " 'using',\n",
       " 'driver',\n",
       " 'understanding',\n",
       " 'encouraged',\n",
       " 'throughout',\n",
       " 'journey',\n",
       " 'recommended',\n",
       " 'write',\n",
       " 'reviews',\n",
       " 'know',\n",
       " 'defiantly',\n",
       " 'recommend',\n",
       " 'instagram',\n",
       " 'name',\n",
       " 'teekezi',\n",
       " 'wish',\n",
       " 'get',\n",
       " 'contact',\n",
       " 'ambassador',\n",
       " 'anything',\n",
       " 'like',\n",
       " 'super',\n",
       " 'happy',\n",
       " 'service',\n",
       " 'receiving']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in simple_preprocess(str(doc)) if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-princeton",
   "metadata": {},
   "source": [
    "## Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "conscious-floor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love bolt. I don‚Äôt use uber often because one ride even if it‚Äôs short is like ¬£11. Whereas from my high street to my house is ¬£3. Not only that but their drivers are SUPER friendly! I was sick one day (my first time using bolt) and the driver was so understanding and encouraged me throughout my journey. Bolt is 100% recommended by me. I don‚Äôt write reviews so that‚Äôs how you know I defiantly recommend it. My Instagram name is: TeeKezi if you wish to get in contact with me about bolt. I‚Äôm not an ambassador üòÇ or anything like that. Just super happy with the service I have been receiving x'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = apple[0]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "rising-equity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love bolt  I don‚Äôt use uber often because one ride even if it‚Äôs short is like ¬£11  Whereas from my high street to my house is ¬£3  Not only that but their drivers are SUPER friendly  I was sick one day  my first time using bolt  and the driver was so understanding and encouraged me throughout my journey  Bolt is 100  recommended by me  I don‚Äôt write reviews so that‚Äôs how you know I defiantly recommend it  My Instagram name is  TeeKezi if you wish to get in contact with me about bolt  I‚Äôm not an ambassador üòÇ or anything like that  Just super happy with the service I have been receiving x'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[%s]' % re.escape(string.punctuation), ' ', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-conducting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "middle-gospel",
   "metadata": {},
   "source": [
    "## Stemming The Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "contrary-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "japanese-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aquatic-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = [word for word in simple_preprocess(str(doc)) if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "statistical-alcohol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love  :  love\n",
      "use  :  use\n",
      "often  :  often\n",
      "one  :  one\n",
      "ride  :  ride\n",
      "even  :  even\n",
      "short  :  short\n",
      "like  :  like\n",
      "whereas  :  wherea\n",
      "high  :  high\n",
      "street  :  street\n",
      "house  :  hous\n",
      "drivers  :  driver\n",
      "super  :  super\n",
      "friendly  :  friendli\n",
      "sick  :  sick\n",
      "one  :  one\n",
      "day  :  day\n",
      "first  :  first\n",
      "time  :  time\n",
      "using  :  use\n",
      "driver  :  driver\n",
      "understanding  :  understand\n",
      "encouraged  :  encourag\n",
      "throughout  :  throughout\n",
      "journey  :  journey\n",
      "recommended  :  recommend\n",
      "write  :  write\n",
      "reviews  :  review\n",
      "know  :  know\n",
      "defiantly  :  defiantli\n",
      "recommend  :  recommend\n",
      "instagram  :  instagram\n",
      "name  :  name\n",
      "teekezi  :  teekezi\n",
      "wish  :  wish\n",
      "get  :  get\n",
      "contact  :  contact\n",
      "ambassador  :  ambassador\n",
      "anything  :  anyth\n",
      "like  :  like\n",
      "super  :  super\n",
      "happy  :  happi\n",
      "service  :  servic\n",
      "receiving  :  receiv\n"
     ]
    }
   ],
   "source": [
    "for x in list_of_words:\n",
    "    print(x, ' : ', stemming.stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-freedom",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-breathing",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "infectious-boundary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\enlik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fallen-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "medium-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = [word for word in simple_preprocess(str(doc)) if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "asian-wallet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love  :  love\n",
      "use  :  use\n",
      "often  :  often\n",
      "one  :  one\n",
      "ride  :  ride\n",
      "even  :  even\n",
      "short  :  short\n",
      "like  :  like\n",
      "whereas  :  whereas\n",
      "high  :  high\n",
      "street  :  street\n",
      "house  :  house\n",
      "drivers  :  driver\n",
      "super  :  super\n",
      "friendly  :  friendly\n",
      "sick  :  sick\n",
      "one  :  one\n",
      "day  :  day\n",
      "first  :  first\n",
      "time  :  time\n",
      "using  :  using\n",
      "driver  :  driver\n",
      "understanding  :  understanding\n",
      "encouraged  :  encouraged\n",
      "throughout  :  throughout\n",
      "journey  :  journey\n",
      "recommended  :  recommended\n",
      "write  :  write\n",
      "reviews  :  review\n",
      "know  :  know\n",
      "defiantly  :  defiantly\n",
      "recommend  :  recommend\n",
      "instagram  :  instagram\n",
      "name  :  name\n",
      "teekezi  :  teekezi\n",
      "wish  :  wish\n",
      "get  :  get\n",
      "contact  :  contact\n",
      "ambassador  :  ambassador\n",
      "anything  :  anything\n",
      "like  :  like\n",
      "super  :  super\n",
      "happy  :  happy\n",
      "service  :  service\n",
      "receiving  :  receiving\n"
     ]
    }
   ],
   "source": [
    "for x in list_of_words:\n",
    "    print(x, ' : ', lemmatizer.lemmatize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-blood",
   "metadata": {},
   "source": [
    "### Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "conceptual-average",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love bolt. I don‚Äôt use uber often because one ride even if it‚Äôs short is like ¬£11. Whereas from my high street to my house is ¬£3. Not only that but their drivers are SUPER friendly! I was sick one day (my first time using bolt) and the driver was so understanding and encouraged me throughout my journey. Bolt is 100% recommended by me. I don‚Äôt write reviews so that‚Äôs how you know I defiantly recommend it. My Instagram name is: TeeKezi if you wish to get in contact with me about bolt. I‚Äôm not an ambassador üòÇ or anything like that. Just super happy with the service I have been receiving x'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "careful-entity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON- love bolt . -PRON- do not use uber often because one ride even if -PRON- ‚Äô short be like ¬£ 11 . whereas from -PRON- high street to -PRON- house be ¬£ 3 . not only that but -PRON- driver be super friendly ! -PRON- be sick one day ( -PRON- first time use bolt ) and the driver be so understanding and encourage -PRON- throughout -PRON- journey . Bolt be 100 % recommend by -PRON- . -PRON- do not write review so that ‚Äô how -PRON- know -PRON- defiantly recommend -PRON- . -PRON- Instagram name be : TeeKezi if -PRON- wish to get in contact with -PRON- about bolt . -PRON- be not an ambassador üòÇ or anything like that . just super happy with the service -PRON- have be receive x\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "sp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "# spacy_doc = sp('testing the best game ever that I ever played')\n",
    "spacy_doc = sp(doc)\n",
    "print (\" \".join([token.lemma_ for token in spacy_doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dramatic-litigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do display word\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "test_doc = nlp('did displaying words')\n",
    "print (\" \".join([token.lemma_ for token in test_doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-accident",
   "metadata": {},
   "source": [
    "## Building Corpus using gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-interest",
   "metadata": {},
   "source": [
    "### Using utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "herbal-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "executive-turkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making bigrams...\n",
      "Lemmatizing...\n"
     ]
    }
   ],
   "source": [
    "cleaned_docs = remove_things(apple)\n",
    "lists_of_words = list(sentences_to_words(cleaned_docs))\n",
    "lists_of_words_no_stops = remove_stopwords(lists_of_words)\n",
    "ngrams = make_bigrams(lists_of_words_no_stops)\n",
    "data_lemmatized = lemmatize(ngrams, allowed_postags=['NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "registered-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "corpus  = [id2word.doc2bow(text) for text in data_lemmatized]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "generous-navigator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('ambassador', 1),\n",
       "  ('contact', 1),\n",
       "  ('day', 1),\n",
       "  ('driver', 2),\n",
       "  ('journey', 1),\n",
       "  ('love', 1),\n",
       "  ('review', 1),\n",
       "  ('ride', 1),\n",
       "  ('understanding', 1),\n",
       "  ('use', 1),\n",
       "  ('write', 1)]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-partnership",
   "metadata": {},
   "source": [
    "### Creating Dictionary using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "smoking-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "behavioral-photography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CNTK', 'formerly', 'known', 'as', 'Computational', 'Network', 'Toolkit'],\n",
       " ['is',\n",
       "  'a',\n",
       "  'free',\n",
       "  'easy-to-use',\n",
       "  'open-source',\n",
       "  'commercial-grade',\n",
       "  'toolkit'],\n",
       " ['that',\n",
       "  'enable',\n",
       "  'us',\n",
       "  'to',\n",
       "  'train',\n",
       "  'deep',\n",
       "  'learning',\n",
       "  'algorithms',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'like',\n",
       "  'the',\n",
       "  'human',\n",
       "  'brain.']]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = [\n",
    "   \"CNTK formerly known as Computational Network Toolkit\",\n",
    "   \"is a free easy-to-use open-source commercial-grade toolkit\",\n",
    "   \"that enable us to train deep learning algorithms to learn like the human brain.\"\n",
    "]\n",
    "\n",
    "text_tokens = [[text for text in doc.split()] for doc in doc]\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "universal-album",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(27 unique tokens: ['CNTK', 'Computational', 'Network', 'Toolkit', 'as']...)\n"
     ]
    }
   ],
   "source": [
    "dict_LoS = corpora.Dictionary(text_tokens)\n",
    "print(dict_LoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "brief-operations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CNTK': 0, 'Computational': 1, 'Network': 2, 'Toolkit': 3, 'as': 4, 'formerly': 5, 'known': 6, 'a': 7, 'commercial-grade': 8, 'easy-to-use': 9, 'free': 10, 'is': 11, 'open-source': 12, 'toolkit': 13, 'algorithms': 14, 'brain.': 15, 'deep': 16, 'enable': 17, 'human': 18, 'learn': 19, 'learning': 20, 'like': 21, 'that': 22, 'the': 23, 'to': 24, 'train': 25, 'us': 26}\n"
     ]
    }
   ],
   "source": [
    "print(dict_LoS.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-pizza",
   "metadata": {},
   "source": [
    "### Creating BoW Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "downtown-trigger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(2, 1), (3, 1), (4, 2)], [(0, 2), (3, 3), (5, 2), (6, 1), (7, 2), (8, 1)]]\n",
      "[[('are', 1), ('hello', 1), ('how', 1), ('you', 1)], [('how', 1), ('you', 1), ('do', 2)], [('are', 2), ('you', 3), ('doing', 2), ('hey', 1), ('what', 2), ('yes', 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pprint\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "doc_list = [\n",
    "   \"Hello, how are you?\", \"How do you do?\", \n",
    "   \"Hey what are you doing? yes you What are you doing?\"\n",
    "]\n",
    "doc_tokenized = [simple_preprocess(doc) for doc in doc_list]\n",
    "dictionary = corpora.Dictionary()\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "print(BoW_corpus)\n",
    "id_words = [[(dictionary[id], count) for id, count in line] for line in BoW_corpus]\n",
    "print(id_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-buying",
   "metadata": {},
   "source": [
    "### (Optional) Get the TF-IDF Corpus Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "looking-florida",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(2, 1), (3, 1), (4, 2)],\n",
       " [(0, 2), (3, 3), (5, 2), (6, 1), (7, 2), (8, 1)]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "blind-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(BoW_corpus)\n",
    "corpus_tfidf = tfidf[BoW_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "caroline-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(BoW_corpus, \n",
    "                                       num_topics=10, \n",
    "                                       id2word = id2word, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "breeding-butler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.000*\"driver\" + 0.000*\"ambassador\" + 0.000*\"journey\" + 0.000*\"day\" + 0.000*\"love\" + 0.000*\"contact\" + 0.000*\"ride\" + 0.000*\"review\" + 0.000*\"understanding\" + 0.000*\"equation\"\n",
      "Topic: 1 \n",
      "Words: 0.000*\"journey\" + 0.000*\"driver\" + 0.000*\"day\" + 0.000*\"ambassador\" + 0.000*\"contact\" + 0.000*\"love\" + 0.000*\"ride\" + 0.000*\"review\" + 0.000*\"understanding\" + 0.000*\"equation\"\n",
      "Topic: 2 \n",
      "Words: 0.000*\"driver\" + 0.000*\"ambassador\" + 0.000*\"journey\" + 0.000*\"day\" + 0.000*\"contact\" + 0.000*\"love\" + 0.000*\"ride\" + 0.000*\"review\" + 0.000*\"understanding\" + 0.000*\"equation\"\n",
      "Topic: 3 \n",
      "Words: 0.000*\"driver\" + 0.000*\"journey\" + 0.000*\"ambassador\" + 0.000*\"day\" + 0.000*\"contact\" + 0.000*\"love\" + 0.000*\"ride\" + 0.000*\"review\" + 0.000*\"understanding\" + 0.000*\"equation\"\n",
      "Topic: 4 \n",
      "Words: 0.007*\"driver\" + 0.007*\"day\" + 0.007*\"journey\" + 0.004*\"contact\" + 0.004*\"ambassador\" + 0.000*\"love\" + 0.000*\"ride\" + 0.000*\"review\" + 0.000*\"understanding\" + 0.000*\"equation\"\n",
      "Topic: 5 \n",
      "Words: 0.000*\"driver\" + 0.000*\"day\" + 0.000*\"journey\" + 0.000*\"ambassador\" + 0.000*\"contact\" + 0.000*\"love\" + 0.000*\"ride\" + 0.000*\"review\" + 0.000*\"understanding\" + 0.000*\"equation\"\n",
      "Topic: 6 \n",
      "Words: 0.000*\"driver\" + 0.000*\"day\" + 0.000*\"journey\" + 0.000*\"ambassador\" + 0.000*\"contact\" + 0.000*\"love\" + 0.000*\"ride\" + 0.000*\"review\" + 0.000*\"understanding\" + 0.000*\"equation\"\n",
      "Topic: 7 \n",
      "Words: 0.000*\"driver\" + 0.000*\"journey\" + 0.000*\"day\" + 0.000*\"ambassador\" + 0.000*\"contact\" + 0.000*\"love\" + 0.000*\"ride\" + 0.000*\"review\" + 0.000*\"understanding\" + 0.000*\"equation\"\n",
      "Topic: 8 \n",
      "Words: 0.010*\"driver\" + 0.007*\"ride\" + 0.007*\"love\" + 0.007*\"ambassador\" + 0.004*\"understanding\" + 0.004*\"review\" + 0.000*\"journey\" + 0.000*\"day\" + 0.000*\"contact\" + 0.000*\"facture_et\"\n",
      "Topic: 9 \n",
      "Words: 0.000*\"driver\" + 0.000*\"journey\" + 0.000*\"day\" + 0.000*\"ambassador\" + 0.000*\"love\" + 0.000*\"contact\" + 0.000*\"ride\" + 0.000*\"review\" + 0.000*\"understanding\" + 0.000*\"equation\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics():\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-yesterday",
   "metadata": {},
   "source": [
    "### Example from https://github.com/fiyero/LDA_gensim/blob/master/LDA%20with%20Gensim_git.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "disabled-green",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('datasets/abcnews-date-text.csv', error_bad_lines=False)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "manual-headline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text = data[:300000][['headline_text']]\n",
    "\n",
    "data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "reported-norman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text['index'] = data_text.index\n",
    "\n",
    "documents = data_text\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "approved-institute",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\enlik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "behavioral-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in stopwords.words('english') and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "given-writer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "document_num = 4310\n",
    "doc_sample = documents[documents['index'] == document_num].values[0][0]\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.items():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_doc_4310 = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpus_tfidf:\n",
    "    print(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=10, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics():\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence=\"u_mass\")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=bow_corpus, texts=processed_docs, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                             num_topics=10, \n",
    "                                             id2word = dictionary, \n",
    "                                             passes = 2, \n",
    "                                             workers=4)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda_idf = CoherenceModel(model=lda_model_tfidf, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_model_lda_idf = coherence_model_lda_idf.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_model_lda_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original Text of sample document 4310\n",
    "processed_docs[document_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[document_num]], key=lambda tup: tup[1], reverse=True):\n",
    "    print(\"\\nScore: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model[bow_corpus[document_num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lda_model[bow_corpus[document_num]], key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topic(index, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[document_num]], key=lambda tup: tup[1], reverse=True):\n",
    "    print(\"\\nScore: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_document = \"My name is Patrick.\"\n",
    "\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: tup[1], reverse=True):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
